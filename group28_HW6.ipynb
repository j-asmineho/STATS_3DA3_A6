{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: STATS 3DA3\n",
    "subtitle: Homework Assignment 6\n",
    "author: \"Ashley Chen, Jasmine Ho, JC Abanto\"\n",
    "date: 04/16/2025\n",
    "format: pdf\n",
    "header-includes:\n",
    "   - \\usepackage{amsmath}\n",
    "   - \\usepackage{bbm}\n",
    "   - \\usepackage{array}\n",
    "   - \\usepackage{multirow}\n",
    "   - \\usepackage{graphicx}\n",
    "   - \\usepackage{float}\n",
    "   - \\usepackage{apacite}\n",
    "   - \\usepackage{natbib}\n",
    "execute: \n",
    "  echo: true\n",
    "fontsize: 11pt\n",
    "geometry: margin = 1in\n",
    "linestretch: 1.5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable measure the severity of heart disease which can be defined as binary classification problem. Our goal would then to be able to predict the probability of heart disease. This means we can carry out a logistic regression and a random forest classifier to predict the presence and absence of heart disease.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "  \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    " \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 13 features and 1 target variable. Starting with our features, we have age (age in years), sex (1 = male; 0 = female), cp (1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic), trestbps (resting blood pressure in mm Hg on admission to the hospital), chol (serum cholestoral in mg/dl), fbs (fasting blood sugar > 120 mg/dl where 1 = true; 0 = false), restecg (resting electrocardiographic results), exang (exercise induced angina), oldpeak (ST depression induced by exercise relative to rest), slope (1 = upsloping, 2 = flat, 3 = downsloping), ca (number of major vessels colored by floursopy), thal (3 = normal; 6 = fixed defect; 7 = reversable defect). Finally our target variable, 'num', is the diagnosis of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Observations in X: {len(X)}\")\n",
    "print(f\"Summary of X:\\n{X.describe()}\")\n",
    "print(f\"Summary of y:\\n{y.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the average age of patients is 54.4 years old, with a standard deviation of 9.1 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Types of X: \\n{X.dtypes}\")\n",
    "print(f\"\\nData Types of y: \\n{y.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data types in X are numerical but some representing categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y before transformation: {y['num'].value_counts()}\")\n",
    "y['num'] = y['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(f\"y before transformation: {y['num'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this correlation matrix, we can conclude that thalach (max heart rate) has a strong negative correlation (-0.39) with age and oldpeak. We can assume that younger individuals tend to have a higher heart rate, while those with more severe heart disease (higher oldpeak) have lower thalach. We also found that ca (number of major vessels) has a strong positive correlation (0.36) with age. We can say that older individuals are more likely to have more blocked vessels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "print(f\"Length of X after transformations: {len(X)}\")\n",
    "print(f\"Length of y after transformations: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume X is your original DataFrame\n",
    "categorical_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "X_cleaned = X.drop(columns=categorical_columns)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2,10)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters = k, n_init = 20, random_state = 0)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.title(\"k Values Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title(\"K-Means Clustering (k=2)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_cleaned.columns)\n",
    "X_scaled_df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) \n",
    "\n",
    "We are going to use logistic regression and random forest. Logistic regression is suitable for this assignment because it predicts a binary outcome, in which case, the target variable is 0 or 1. Random forest is also a good classifier to use because it has high predictive accuracy and does not depend on linear relationships, which is good for this dataset as there are both numerical and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10)\n",
    "\n",
    "We are going to use accuracy and F1 scores to compare the classifier performance between logistic regression and random forest. We can create the confusion matrix from the predictions to calculate the accuracy and F1 scores. Accuracy scores are calculated by the number of correct predictions over the total number of predictions. \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} &= \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n",
    "\\text{Where:} \\\\\n",
    "TP &= \\text{True Positives} \\\\\n",
    "TN &= \\text{True Negatives} \\\\\n",
    "FP &= \\text{False Positives} \\\\\n",
    "FN &= \\text{False Negatives}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "To obtain the F1 score, we will need to use precision and recall that are derived from the confusion matrix. Once we calculate that, the F1 score can be calculated by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} &= \\frac{TP}{TP + FP} \\\\\n",
    "\\text{Recall} &= \\frac{TP}{TP + FN} \\\\\n",
    "F_1 &= \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}\n",
    "\n",
    "Using these metrics, we can compare the overall accuracy and balance between the two classifiers to determine which classifier most optimal for predicting heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] # test this out to see if accuracy improves\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "y_pred = best_log_reg.predict(X_test_scaled)\n",
    "y_pred2 = best_log_reg.predict(X_train_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_pred2)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['No Heart Disease', 'Heart Disease'])\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "print(f\"Train set accuracy: {train_accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(estimator=rf_clf, param_grid=rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Random Forest Best Parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Random Forest Best Cross-Validation Accuracy:\", rf_grid_search.best_score_)\n",
    "\n",
    "best_rf_clf = rf_grid_search.best_estimator_\n",
    "rf_y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_cm = confusion_matrix(y_test, rf_y_pred)\n",
    "rf_report = classification_report(y_test, rf_y_pred, target_names=[f'Class {i}' for i in np.unique(y)])\n",
    "\n",
    "print(f\"Random Forest Test Accuracy: {rf_accuracy:.4f}\")\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(rf_cm)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(rf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage \n",
    "\n",
    "### References\n",
    "\n",
    "Stack Overflow. (2014, February 16). Fine-tuning parameters in logistic regression. Stack Overflow. https://stackoverflow.com/questions/21816346/fine-tuning-parameters-in-logistic-regression\n",
    "\n",
    "Stack Overflow. (2016, July 17). Random Forest Hyperparameter Tuning - scikit-learn using GridSearchCV. Stack Overflow. https://stackoverflow.com/questions/35164310/random-forest-hyperparameter-tuning-scikit-learn-using-gridsearchcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
